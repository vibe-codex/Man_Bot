#!/usr/bin/env python3
"""
–û—á–∏—Å—Ç–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π –∞—É–¥–∏–æ.

–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç:
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
- OCR-–æ—à–∏–±–∫–∏ (–∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞)
- –†–∞–∑—Ä—ã–≤—ã —Ç–µ–∫—Å—Ç–∞
- –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python clean_transcription.py --input transcriptions_raw --output transcriptions_clean
"""

import re
import argparse
from pathlib import Path
from typing import List, Dict, Any


class TranscriptionCleaner:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    """
    
    def __init__(self):
        # –°–ª–æ–≤–∞—Ä—å —Ç–∏–ø–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫ OCR/—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏
        self.common_errors = {
            # –ó–≤—É–∫–æ–≤—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            r'\[–º—É–∑—ã–∫–∞\]': '',
            r'\[—à—É–º\]': '',
            r'\[–Ω—Ä–∑–±\]': '',
            r'\(–∑–≤—É–∫\)': '',
            r'\(–∞–ø–ª–æ–¥–∏—Å–º–µ–Ω—Ç—ã\)': '',
            r'\(—Å–º–µ—Ö\)': '',
            r'\.\.\.': ' ',  # –ú–Ω–æ–≥–æ—Ç–æ—á–∏—è –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
            r'  +': ' ',  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        }
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤
        self.word_fixes = {
            '–¥–µ–Ω—å–∂–∞—Ç–∞': '–¥–µ–Ω—å–≥–∏',
            '–≤–æ–æ–±—â–µ–º': '–≤ –æ–±—â–µ–º',
            '–≤–æ–±—â–µ–º': '–≤ –æ–±—â–µ–º',
            '–∫–æ—Ä–æ—á–µ': '–∫–æ—Ä–æ—á–µ',
            '—Ç–∏–ø–∞': '—Ç–∏–ø–∞',
            '–∫–∞–∫ –±—ã': '–∫–∞–∫ –±—ã',
            '–Ω—É —Ç–∏–ø–∞': '–Ω—É —Ç–∏–ø–∞',
            '–∫—Å—Ç–∞': '–∫—Å—Ç–∞—Ç–∏',
            '–ø–ª–∏–∑': '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞',
            '—Å–ø—Å': '—Å–ø–∞—Å–∏–±–æ',
            '–ø—Ä–∏–≤': '–ø—Ä–∏–≤–µ—Ç',
            '–ø–æ–Ω': '–ø–æ–Ω—è—Ç–Ω–æ',
            '–Ω–µ –ø–æ–Ω': '–Ω–µ –ø–æ–Ω—è—Ç–Ω–æ',
            '—á–µ—Ç': '—á—Ç–æ-—Ç–æ',
            '–∫–æ—Ä–æ—á': '–∫–æ—Ä–æ—á–µ',
        }
    
    def _remove_artifacts(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
        for pattern, replacement in self.common_errors.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        return text.strip()
    
    def _fix_words(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤"""
        words = text.split()
        fixed_words = []
        
        for word in words:
            # –û—á–∏—â–∞–µ–º –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            clean = word.lower().strip('.,!?;:')
            if clean in self.word_fixes:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–≥–∏—Å—Ç—Ä –ø–µ—Ä–≤–æ–π –±—É–∫–≤—ã
                if word[0].isupper():
                    fixed = self.word_fixes[clean].capitalize()
                else:
                    fixed = self.word_fixes[clean]
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
                if word[-1] in '.,!?;:':
                    fixed += word[-1]
                fixed_words.append(fixed)
            else:
                fixed_words.append(word)
        
        return ' '.join(fixed_words)
    
    def _add_punctuation(self, text: str) -> str:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ –∏ —Å–æ—é–∑–∞–º
        words = text.split()
        if not words:
            return text
        
        sentences = []
        current = []
        
        for i, word in enumerate(words):
            current.append(word)
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if i < len(words) - 1:
                next_word = words[i + 1].lower()
                
                # –ö–æ–Ω–µ—Ü –ø–µ—Ä–µ–¥ —Å–æ—é–∑–æ–º (–Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
                if next_word in ['–∞', '–Ω–æ', '–∏', '–∏–ª–∏', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É']:
                    if len(current) >= 3:
                        sentences.append(' '.join(current))
                        current = []
                
                # –ö–æ–Ω–µ—Ü –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
                if next_word in ['–¥–∞', '–Ω–µ—Ç', '–∫–æ–Ω–µ—á–Ω–æ'] and len(current) >= 2:
                    sentences.append(' '.join(current))
                    current = []
        
        if current:
            sentences.append(' '.join(current))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        result = []
        for sent in sentences:
            sent = sent.strip()
            if not sent:
                continue
            
            lower = sent.lower()
            
            # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ
            if any(w in lower for w in ['—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '–∫–∞–∫', '–∫–∞–∫–æ–π', '–∫–æ—Ç–æ—Ä—ã–π']):
                if not sent.endswith('?'):
                    sent += '?'
            # –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ
            elif any(w in lower for w in ['–≤–∞—É', '–æ–≥–æ', '–∫—Ä—É—Ç–æ', '—Å—É–ø–µ—Ä', '–±–æ–∂–µ', '—á–µ—Ä—Ç', '–æ–≥–æ']):
                if not sent.endswith('!'):
                    sent += '!'
            # –û–±—ã—á–Ω–æ–µ
            else:
                if not sent[-1] in '.!?':
                    sent += '.'
            
            result.append(sent)
        
        return ' '.join(result)
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∞–±–∑–∞—Ü—ã –ø–æ —Å–º—ã—Å–ª–æ–≤—ã–º –±–ª–æ–∫–∞–º"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–º –ø–∞—É–∑–∞–º
        paragraphs = re.split(r'\n\s*\n|\n{2,}', text)
        return [p.strip() for p in paragraphs if p.strip()]
    
    def process_file(self, input_path: Path, output_path: Path):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        print(f"üéôÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞: {input_path.name}")
        
        # –ß–∏—Ç–∞–µ–º
        text = input_path.read_text(encoding='utf-8')
        original_len = len(text)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        text = self._remove_artifacts(text)
        text = self._fix_words(text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π
        paragraphs = self._split_paragraphs(text)
        processed = []
        
        for i, para in enumerate(paragraphs):
            print(f"  üìÑ –ê–±–∑–∞—Ü {i+1}/{len(paragraphs)}")
            para = self._add_punctuation(para)
            processed.append(para)
        
        # –°–æ–±–∏—Ä–∞–µ–º
        final_text = '\n\n'.join(processed)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        header = f"""# –û—á–∏—â–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
## –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {input_path.name}
## –ê–±–∑–∞—Ü–µ–≤: {len(paragraphs)}
## –°–∏–º–≤–æ–ª–æ–≤: {original_len} ‚Üí {len(final_text)}

---

"""
        
        full_text = header + final_text
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output_path.write_text(full_text, encoding='utf-8')
        
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
        return len(paragraphs)
    
    def process_directory(self, input_dir: Path, output_dir: Path):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–π –ø–∞–ø–∫–∏"""
        output_dir.mkdir(exist_ok=True)
        
        txt_files = list(input_dir.glob('*.txt'))
        if not txt_files:
            print(f"‚ö†Ô∏è  –ù–µ—Ç .txt —Ñ–∞–π–ª–æ–≤ –≤ {input_dir}")
            return
        
        print(f"\nüéØ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(txt_files)}\n")
        
        total_paragraphs = 0
        for i, txt_file in enumerate(txt_files, 1):
            print(f"\n[{i}/{len(txt_files)}]")
            output_file = output_dir / f"clean_{txt_file.name}"
            try:
                paras = self.process_file(txt_file, output_file)
                total_paragraphs += paras
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        print("\n" + "="*50)
        print("üìä –ò–¢–û–ì–ò:")
        print(f"  –§–∞–π–ª–æ–≤: {len(txt_files)}")
        print(f"  –ê–±–∑–∞—Ü–µ–≤: {total_paragraphs}")
        print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_dir}")
        print("="*50 + "\n")


def main():
    parser = argparse.ArgumentParser(description='–û—á–∏—Å—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–π –∞—É–¥–∏–æ')
    parser.add_argument('--input', '-i', default='transcriptions_raw',
                       help='–ü–∞–ø–∫–∞ —Å —Å—ã—Ä—ã–º–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è–º–∏')
    parser.add_argument('--output', '-o', default='transcriptions_clean',
                       help='–ü–∞–ø–∫–∞ –¥–ª—è –æ—á–∏—â–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
    
    args = parser.parse_args()
    
    input_dir = Path(args.input)
    output_dir = Path(args.output)
    
    if not input_dir.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {input_dir}")
        print("–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É –∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ .txt —Ñ–∞–π–ª—ã")
        return
    
    cleaner = TranscriptionCleaner()
    cleaner.process_directory(input_dir, output_dir)
    
    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ —Ä–∞–∑–º–µ—á–∞—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ Kimi.")


if __name__ == "__main__":
    main()